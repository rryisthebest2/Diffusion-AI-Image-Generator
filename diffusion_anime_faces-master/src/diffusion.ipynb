{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "ZqK-f_m7iXfG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672174092903,
     "user_tz": 300,
     "elapsed": 2699,
     "user": {
      "displayName": "chen yidi",
      "userId": "02204373396816677160"
     }
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from typing import List, Tuple, Union, Optional, Callable\n",
    "import math\n",
    "from abc import ABC\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from diffusion import Diffusion\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class PILImageDataset(torch.utils.data.Dataset, ABC):\n",
    "    directory: str\n",
    "    files: List[str]\n",
    "    cache: List\n",
    "    tsfm: Callable\n",
    "\n",
    "    def __init__(self, directory: str, tsfm: Callable):\n",
    "        self.directory = directory\n",
    "        self.files = [directory + '/' + i for i in listdir(self.directory)]\n",
    "        # self.cache = [None] * len(self.files)\n",
    "        self.tsfm = tsfm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # if self.cache[idx] is not None:\n",
    "        #     return self.cache[idx]\n",
    "        img = Image.open(self.files[idx])\n",
    "        # self.cache[idx] = self.tsfm(img)\n",
    "        res = self.tsfm(img)\n",
    "        img.close()\n",
    "        return res, torch.Tensor([])"
   ],
   "metadata": {
    "id": "VhpCulKbQ0QO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672174092904,
     "user_tz": 300,
     "elapsed": 9,
     "user": {
      "displayName": "chen yidi",
      "userId": "02204373396816677160"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "\n",
    "MODEL_SAVE_PATH = \"../model/\""
   ],
   "metadata": {
    "id": "cCyDPZjJiXfJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672174093132,
     "user_tz": 300,
     "elapsed": 6,
     "user": {
      "displayName": "chen yidi",
      "userId": "02204373396816677160"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: './data/anime_icons'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m DATA_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data/anime_icons\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      2\u001B[0m data_transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      3\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m64\u001B[39m)),\n\u001B[0;32m      4\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m      5\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize(\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m),\n\u001B[0;32m      6\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mRandomHorizontalFlip()\n\u001B[0;32m      7\u001B[0m ])\n\u001B[1;32m----> 9\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mPILImageDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtsfm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_transform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m data_loader_train \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mPILImageDataset.__init__\u001B[1;34m(self, directory, tsfm)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, directory: \u001B[38;5;28mstr\u001B[39m, tsfm: Callable):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdirectory \u001B[38;5;241m=\u001B[39m directory\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfiles \u001B[38;5;241m=\u001B[39m [directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdirectory\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# self.cache = [None] * len(self.files)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtsfm \u001B[38;5;241m=\u001B[39m tsfm\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] 系统找不到指定的路径。: './data/anime_icons'"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./data/anime_icons\"\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])\n",
    "\n",
    "dataset = PILImageDataset(DATA_PATH, tsfm=data_transform)\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "betas = torch.linspace(0.0001, 0.02, num_steps)\n",
    "device = torch.device(\"cuda\")\n",
    "num_epoch = 100"
   ],
   "metadata": {
    "id": "vFTG5w9miXfM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672174093132,
     "user_tz": 300,
     "elapsed": 6,
     "user": {
      "displayName": "chen yidi",
      "userId": "02204373396816677160"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = UNet(3, 3, resolute_multiplication=(1, 2, 2, 2),\n",
    "             is_attention=(False, False, False, True)).to(device)\n",
    "model.load_state_dict(torch.load(f\"{MODEL_SAVE_PATH}/model\"))\n",
    "optim = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "diffusion = Diffusion(model, optim, betas, device)"
   ],
   "metadata": {
    "id": "K4ae9z0fiXfM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672174104302,
     "user_tz": 300,
     "elapsed": 11175,
     "user": {
      "displayName": "chen yidi",
      "userId": "02204373396816677160"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "img_tsfm = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])\n",
    "\n",
    "def generate_by(img_path: str, times: int = 10,\n",
    "                frm: Callable[[int], int] = lambda x: 300, stp: int = 10) -> None:\n",
    "  img = img_tsfm(Image.open(img_path)).to(device).view(1, 3, 64, 64)\n",
    "  imgs = [img]\n",
    "  for i in range(1, times + 1):\n",
    "    print(f\"\\r{i}\", end=\"\")\n",
    "    frming = frm(i)\n",
    "    img_gen = diffusion.generate_from(img, frming)\n",
    "    save_image(\n",
    "        make_grid(torch.cat(img_gen[-frming::stp], dim = 0) * 0.5 + 0.5, value_range=(-1, 1), nrow=4),\n",
    "        f\"diffusion_{i}.png\"\n",
    "    )\n",
    "    save_image(img_gen[-1] * 0.5 + 0.5, f\"generated_{i}.png\")\n",
    "    imgs.append(img_gen[-1])\n",
    "    img = img_gen[-1]\n",
    "  print()\n",
    "  save_image(\n",
    "      make_grid(torch.cat(imgs, dim = 0) * 0.5 + 0.5, value_range=(-1, 1), nrow=4),\n",
    "      f\"generating_process.png\"\n",
    "  )"
   ],
   "metadata": {
    "id": "f6xYQrZ2TDI6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672174104303,
     "user_tz": 300,
     "elapsed": 22,
     "user": {
      "displayName": "chen yidi",
      "userId": "02204373396816677160"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "\n",
    "def train():\n",
    "    for k in range(1, num_epoch + 1):\n",
    "        loss_avg = 0.\n",
    "\n",
    "        tot = len(data_loader_train)\n",
    "        idx = 0\n",
    "\n",
    "        for img, lbl in data_loader_train:\n",
    "            loss = diffusion.train(img)\n",
    "            loss_avg += loss\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "            print(f\"\\r{k}: {idx}/{tot}; {loss}\", end=\"\")\n",
    "\n",
    "        print(f\"\\r{k}: {idx}/{tot}; {loss_avg / len(data_loader_train)}\")\n",
    "\n",
    "        xh = diffusion.sample((8, 3, 64, 64))\n",
    "        grid = make_grid(torch.cat([img[:8].to(device), xh[-1]], dim=0) * 0.5 + 0.5, value_range=(-1, 1), nrow=4)\n",
    "        save_image(grid, f\"./ddpm_sample_{k}.png\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{MODEL_SAVE_PATH}/model\")\n",
    "\n",
    "def gen():\n",
    "    xh = diffusion.sample((16, 3, 64, 64), prog=True)\n",
    "    grid = make_grid(xh[-1].to(device) * 0.5 + 0.5, value_range=(-1, 1), nrow=4)\n",
    "    save_image(grid, f\"./ddpm_sample.png\")"
   ],
   "metadata": {
    "id": "OwGNbPnSiXfN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d5a65a90-7df9-4fcb-c2b4-1b86b7c28d6d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/1000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mgen\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36mgen\u001B[1;34m()\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgen\u001B[39m():\n\u001B[1;32m---> 28\u001B[0m     xh \u001B[38;5;241m=\u001B[39m \u001B[43mdiffusion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m     grid \u001B[38;5;241m=\u001B[39m make_grid(xh[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.5\u001B[39m, value_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m), nrow\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m     30\u001B[0m     save_image(grid, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./ddpm_sample.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Programming\\pytorch\\diffusion_anime_head\\src\\diffusion.py:107\u001B[0m, in \u001B[0;36mDiffusion.sample\u001B[1;34m(self, shape, prog)\u001B[0m\n\u001B[0;32m    105\u001B[0m cnt \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps)):\n\u001B[1;32m--> 107\u001B[0m     cur_x \u001B[38;5;241m=\u001B[39m \u001B[43m_diffusion_back\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_minus_alpha_prod_sqrt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m                            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposterior_variance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m     seq\u001B[38;5;241m.\u001B[39mappend(cur_x)\n\u001B[0;32m    110\u001B[0m     cnt \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\Programming\\pytorch\\diffusion_anime_head\\src\\diffusion.py:32\u001B[0m, in \u001B[0;36m_diffusion_back\u001B[1;34m(model, x, tm, beta, one_minus_alpha_prod_sqrt, post_variance, device, end)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_diffusion_back\u001B[39m(model: nn\u001B[38;5;241m.\u001B[39mModule, x: Tensor, tm: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     29\u001B[0m                     beta: torch\u001B[38;5;241m.\u001B[39mTensor, one_minus_alpha_prod_sqrt: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m     30\u001B[0m                     post_variance: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m     31\u001B[0m                     device: torch\u001B[38;5;241m.\u001B[39mdevice, end: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m---> 32\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m     eps_theta \u001B[38;5;241m=\u001B[39m model(x, t)\n\u001B[0;32m     34\u001B[0m     coeff \u001B[38;5;241m=\u001B[39m beta[t] \u001B[38;5;241m/\u001B[39m one_minus_alpha_prod_sqrt[t]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gen()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
